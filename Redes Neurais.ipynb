{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURAIS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em contraste com a Regressão logística, as redes neurais têm tanto seu uso, quanto a sua concepção, um pouco mais recentes. Seus primeiros algoritmos foram definidos por volta da década de 40 e publicados em um artigo escrito por Warren McCulloch do MIT, neurocientista e psiquiatra, e Walter Pitts, matemático da Universidade do Illinois. O artigo explicava a simulação de um neurônio, tentando imitar uma conexão sináptica e, apesar de sua boa repercussão, houve poucos avanços na área durante o período: apenas alguns autoajustes dos pesos e a formulação de Perceptron, na década seguinte. \n",
    "\n",
    "Apesar do método ser promissor, houve um desgaste com a pesquisa do tema, pois várias pesquisas levavam a previsões pouco performáticas e confiáveis. Não obstante, as barreiras de processamento computacional e os problemas na confiabilidade das pesquisas também contribuíram para o lento avanço durante mais de 20 anos. Foi, nos 70, que avanços significativos voltaram a acontecer, graças a alguns pesquisadores que mantiveram maior comprometimento na área. Dentre estes, um dos maiores contribuidores dessa época foi o pesquisador Kunihiko Fukushima, que trouxe pela primeira vez uma pesquisa relevante sobre as Redes Neurais Multicamadas: PMC – Perceptron de Multicamadas (1975), em que demostra a possibilidade de previsão em mais de uma neurônio de saída, diferente do modelo de uma única camada.  Tal avanço possibilitou a previsão além das situações binárias. (SAS, 2020)   \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*piYTTh83qsQJVUMOZKmN5w.png\">\n",
    "</div>\n",
    "\n",
    "A rede neural demonstra seu poder, ao utilizar sua capacidade de reconhecer padrões (treinamento) e incorporar uma generalização de um banco de dados para casos futuros (predição) e de classificação, abordando-os de forma semelhante ao cérebro humano. Dentre estas, há a alta capacidade de expor as generalizações, para quando os modelos de previsão demonstram características não lineares.\n",
    "\n",
    "O conceito básico sobre os funcionamentos de um Neurônio biológico, pode nos auxiliar para a completa compreensão das Redes Neurais Artificiais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O cérebro humano\n",
    "---\n",
    "\n",
    "Em resumo, o sistema nervoso humano é constituído de três fases, como é apresentado no diagrama por AIRBIB (1987, apud. HAYKIN. 2017).\n",
    "O quadro central, citado como Rede Neural, é a representação do Cérebro, no qual recebe informação de maneira continua, absorve e toma as decisões que julga apropriada. As setas para a direita indicam a transmissão dos sinais e processamentos gerados pelos estímulos, enquanto as setas que se direcionam da direita para a esquerda, são os feedbacks gerados para o sistema.\n",
    "Os receptores convertem os estímulos gerados pelo corpo ou pelo ambiente externo em impulsos elétricos que transmitem informações que são absorvidas pelas redes neurais, enquanto a molécula efetora converte os impulsos providos pela rede neural em resposta discernível para o sistema nervoso.\n",
    "\n",
    "Em suma, o cérebro humano é principal responsável pelas funções cognitivas e função sensório motoras autônomas. Além de ter a capacidade de reconhecer padrões, relacioná-los armazenar conhecimentos e utilizá-los para interpretas observações. Sendo que apesar de diversos avanços nas pesquisas de Neurociência, ainda não há um completo conhecimento sobre como funciona o Cérebro. Porém, o comportamento individual de neurônios é conhecido o suficiente para uso funcional. E é exatamente nesses conceitos que se baseiam as Redes Neurais Artificiais.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Neurônio\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo DAMÁSIO (1996, apud SILVA, 2012), o neurônio biológico também é constituído pelas seguintes seções: uma fibra celular de entrada, os dendritos; uma fibra celular de saída, o axônio; e os atuadores. Cada um com suas devidas funções.\n",
    "\n",
    "Com as seguintes capacidades: \n",
    "- Dendritos - recebe os estímulos provenientes de outros neurônios.\n",
    "- Corpo celular - coleta e combina as informações repassadas por outros neurônios.\n",
    "- Axônio - transmiti os estímulos para a célula seguinte.\n",
    "- Contatos sinápticos - são os responsáveis por transmitir a informação de um neurônio para o outro.\n",
    "\n",
    "Portanto, no aspecto biológico, quando uma quantidade de informações passada por meio dos dendritos se acumular no corpo celular atingindo um certo limite, o neurônio envia um sinal eletroquímico ao próximo neurônio conectado, através do axônio.  A extremidade do axônio contém ramificações que se conectam aos dendritos do neurônio seguinte.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://i.pinimg.com/originals/3e/12/12/3e121273af168d94f6221a3d82c6cbea.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Neurais Artificiais\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As redes neurais artificiais tem como finalidade, tentar mimetizar o comportamento estrutural do cérebro até onde temos conhecimento de seu funcionamento e, de qualquer forma, não havendo toda a complexidade de neurônios, tanto em funcionamento, quanto em quantidade, que nos depararmos ao comparar com o cérebro biológico, que segundo a neurocientista: Suzana Herculano, contém um conjunto de aproximadamente 86 bilhões de neurônios. \n",
    "\n",
    "Diversos pesquisadores sugerem estruturas e algoritmos de aprendizado diferentes, de forma que possuam certas características de sistemas biológicos. \n",
    "\n",
    "O modelo de neurônio artificial sugerido por McCulloch e Pitts, por exemplo, é uma simplificação do conhecimento que se tinha na época sobre o funcionamento dos neurônios biológicos. O que se sabia até então: os neurônios recebiam múltiplas entradas excitatórias ou inibitórias dos neurônios anteriores e, caso a soma dessas excitações e inibições ultrapasse um determinado threshold, o neurônio, através do axônio, enviava um impulso nervoso. Porém, a principal desvantagem do modelo proposto por McCulloch-Pitts é o fato de que os inputs processados deveriam ser binarias, e sua saída também era limitada a um output binário. \n",
    "\n",
    "O conceito matemático do Perceptron de Rosenblatt (o conceito que será aplicado nessa pesquisa, por ser mais robusto), resulta em um modelo com n terminais de entrada (dendritos) que recebem os valores $x_1,x_2,…,x_n$ (que representam novos inputs ou o output dos neurônios anteriores) e com base nos pesos de recepção da camada de entrada. Esse input é processado por uma função de combinação que soma os valores $w_i x_i$. O valor resultante é comparado com um threshold definido pela função de ativação do neurônio. Se a soma exceder o valor limitado, o neurônio enviara o output para o terminal de saída $y$ (representando o axônio). Para representar o comportamento das sinapses, os terminais de entrada dos neurônios têm pesos acoplados $w_1,w_2,…,w_n$ cujos valores podem ser positivos ou negativos, dependendo das sinapses correspondentes serem inibitórias ou excitatórias. O efeito de uma sinapse particular $i$ no neurônio pôs sináptico é definido pelo produto $w_i x_i$. Os pesos determinam “em que grau” o neurônio deve considerar sinais de disparo que ocorrem naquela conexão.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron-768x427.png.webp\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de ativação é responsável pela saída y do neurônio a partir de dos valores dos vetores de peso $(\\overrightarrow{w})^{t}=(w_1,w_2,…,w_n )^t$ e de entrada $(\\overrightarrow{x})^t=(x_1,x_2,…,x_n )^t.$  A função de aditivadade de um perceptron faz a soma dos pesos multiplicados pelos inputs mais a soma dos vícios $(b)$, após a soma desses valores, é feita uma comparação com o limitar definido, para decidir se passará a informação adiante ou não.\n",
    "\n",
    "$$a=\\sum_{i=1}^{n}w_ix_i+b$$ \n",
    "\n",
    "Feita a soma, incluímos os valores da adição a função de ativação, como denotada  $f({x})$, ela define a saída do neurônio, em termos dos valores de entrada. A seguir, duas funções de ativação distintas, para exemplificação.\n",
    "  \n",
    "$$\n",
    "y=\n",
    "  \\begin{cases}\n",
    "    1 & \\text{se, a >= 0} \\newline\n",
    "    0 & \\text{se, a < 0} \n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa forma foi apresentada por Rosenblat, que foi o suficiente para atrais diversos entusiadas para a pesquisa da área, porém ela ainda é uma forma muito simples de lidar com questões mais complexas. Seu conceito é robusto mas sua aplicação na forma apresentada pode não demonstrar todo o potencial das Redes Neurais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplificação de funcionamento do Perceptron \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos pegar um exemplo onde estamos fazendo reconhecimento de imagem, para conseguirmos demonstrar de maneira visual seu funcionamento. \n",
    "\n",
    "Então na imagem abaixo iremos tentar reconhecer se um número descrito em uma folha de papel é o número oito ou não, e caso não seja, qual número o método chutaria. \n",
    "\n",
    "Então vamos lá, uma imagem é formada de diversos pixeis, e com essa não seria diferente. Então nossa pequena imagem consiste de 28 pixeis verticais e 28 pixeis horizontais, sendo ao total 784 pixeis. \n",
    "\n",
    "Para cada pixel, nós teremos uma entrada com um valor de 0 a 1, de acordo com a intensidade da escrita naquele ponto da folha de papel. Nessa situação, cada píxel é um input $(x_i)$, e esse valor de entrada será multiplicado pelo peso sináptico $(w_i)$ a fim de quantificar a importancia de cada input em relação ao objetivo funcional do estudo. Os valores para esses pesos são inicialmente aleatórios. E mais pra frente iremos explicar como ajustamos esses valores para otimizarmos nosssas previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://ml4a.github.io/images/figures/mnist-input.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o imput inicial, cada input irá passar por diversos neurônios diferentes, com pesos diferentes e com combinações de inputs diferentes para nossa rede tentar acertar. \n",
    "\n",
    "Ou seja, isso resultada na nossa função de ativação, onde o valor da composição de todas as entradas já ponderas pelo seus pesos mais a soma do vício $(b)$, é repassado como parametros para a função de ativação $(g(a))$ cujo resultado será o output do perceptron $(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://achintavarna.files.wordpress.com/2017/11/mnist_2layers.png?w=634\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De maneira matemática, o processamento do Perceptron basicamente se resumo a essas duas funções: \n",
    "    \n",
    "$$\n",
    "  \\begin{cases}\n",
    "    a = \\sum_{i=1}^{n}w_i \\cdot x_i - b \\newline\n",
    "    y = g(a) \n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Sendo $a$ a soma dos pesos sinápticos e os inputs e o nossa função $g(.)$ representando nossa função sigmóide que passará por um limiar definido. \n",
    "\n",
    "Agpra retornando um pouco aos conceitos do método, iremos nos atentar sobre os como funciona os pesos sinápticos, já que eles são um instrumento essencial para um bom funcionamento da rede. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Treinamento dos Pesos sinápticos \n",
    "--- \n",
    "\n",
    "Existem diversas formas de ser feito o ajustes dos pesos sinapticos, porém uma forma muito facil de contextualização é o método de Hebb, um método bem simples em sua essêcia, mas que apresenta bons resultados em relação a sua dificuldade de aplicação. E seu conceito será traçado de maneira paralela ao Backpropagation, que por enquanto não será aplicado. \n",
    "\n",
    "Mas em sintese, o método de Hebb consiste em verificar se a saída gerada pela rede resulta no valor desejado, caso isso ocorra, os valores dos pesos e o limiar de cada neurônio permancerá o mesmo, caso contrário eles são atualizados proporcionalemnte aos valores dos inputs. \n",
    "\n",
    "Esse processo é repetido para todos os elementos do treinamento, até que o resultado gerado pela redes seja semelhante ao resultado desejado. Os ajustes para os pesos $w_i$ e para os limiares $b$ pode ser expresa da seguinte forma: \n",
    "\n",
    "$\n",
    "  \\begin{cases}\n",
    "    w_{i}^{(atual)} = w_{i}^{(anterior)} + \\alpha \\cdot (d^{(k)} - y) \\cdot x^{(k)} \\newline\n",
    "    b_{i}^{(atual)} = b_{i}^{(anterior)} + \\alpha \\cdot (d^{(k)} - y) \\cdot (-1)\n",
    "  \\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde: \n",
    "- w: Vetor contendo o limiar e os pesos;\n",
    "- $x^{(k)}$: k-ésima amostra de treinamento;\n",
    "- $d^{(k)}$: é o valor desejado para a k-ésima amostra de treinamento;\n",
    "- $y$: é o valor de saída produzido pela rede;\n",
    "- $\\alpha$: é uma constante positiva que defina a taxa de aprendizado.\n",
    "\n",
    "Relembrando que é necessário cautela ao ajustas o valor para a taxa de aprendizado, pois caso contrário podemos esbarrar em problemas para otimizar nossos parametros e isso irá nos trazer diversos problemas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas com isso já seria possível fazer uma aplicação simples dos conhecimentos adquiridos até o momento. \n",
    "\n",
    "Como os passos são simples, é possível expressá-los no pseudo-código abaixo: \n",
    "\n",
    "* <1> Obter o conjunto de amostras de treinamento {$x^{(k)}$};  \n",
    "* <2> Associar a saída desejada {$d^{(k)}$} para cada amostra obtida; \n",
    "* <3> Iniciar o vetor **w** com valores aleatórios pequenos; \n",
    "* <4> Especificar a taxa de aprendizado {$\\alpha$}\n",
    "* <5> Inicar o contador de número de época {época $\\gets$ 0}\n",
    "* <6> Repetir as instruções:  \n",
    "    * <6.1> erro $\\gets$ \"inexiste\";\n",
    "    * <6.2> Para todas as amostras de treinamento {$x^{(k)}$, $d^{(k)}$}, fazer: \n",
    "        * <6.2.1> $u \\gets w^T \\cdot x^{(k)}$; \n",
    "        * <6.2.2> $y \\gets sinal(u)$; \n",
    "        * <6.2.3> Se $y \\ne d^{(k)}$:\n",
    "            * <6.2.3.1> Então $ \n",
    "            \\begin{cases}\n",
    "               w \\gets w+ \\alpha \\cdot (d^{(k)} - y) \\cdot x^{(k)} \\newline\n",
    "               erro \\gets \\text{\"existe\"}\n",
    "            \\end{cases}$\n",
    "    * <6.3> época $\\gets$ época + 1;\n",
    "    Até que erro = \"inexiste\"\n",
    "* **FIM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo essas passos nos levam a uma aplicação pratica relativamente eficaz das redes neurais. Principalmente para situações de classificações binárias lineares, onde é possível fazer um distição dos grupos atravéz de uma reta. \n",
    "\n",
    "Caso não seja claro o motivo dessa situação, é só olharmos para nosso soma para a função de ativação, sendo: \n",
    "\n",
    "$$a = \\sum_{i=1}^{n}w_i \\cdot x_i - b $$\n",
    "\n",
    "Que nos leva a:\n",
    "\n",
    "$$\n",
    "y=\n",
    "  \\begin{cases}\n",
    "    1, & \\text{se  } \\sum_{i=1}^{n}w_i \\cdot x_i - b \\ge 0 \\iff w_1 \\cdot x_1  + w_2 \\cdot x_2 + ... + w_n \\cdot x_n - b \\ge 0\n",
    "    \\newline\n",
    "    0 & \\text{se  }   \\sum_{i=1}^{n}w_i \\cdot x_i - b < 0 \\iff w_1 \\cdot x_1  + w_2 \\cdot x_2 + ... + w_n \\cdot x_n - b < 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Para um aspecto mais simples a fim de podermos visualizar, pegue-mos um caso univariado:\n",
    "\n",
    "$$\n",
    "y=\n",
    "  \\begin{cases}\n",
    "    1, & \\text{se  } \\sum_{i=1}^{2}w_i \\cdot x_i - b \\ge 0 \\iff w_1 \\cdot x_1  + w_2 \\cdot x_2 - b \\ge 0\n",
    "    \\newline\n",
    "    0 & \\text{se  }   \\sum_{i=1}^{2}w_i \\cdot x_i - b < 0 \\iff w_1 \\cdot x_1  + w_2 \\cdot x_2 - b < 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Que pode resultar em diversas retas divisoras, como na imagem abaixo:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://qph.fs.quoracdn.net/main-qimg-f0e7727bb42e1a8ec68df8e256f4d200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O Perceptron de Multicamada \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até o momento, compreendemos o funcionamento do perceptron de Rosenblatt e o ajuste dos pesos sinápticos, mas como já citado anteriormente, podemos aplicar mais algumas complexidades aos conceitos explorados até o momento, de forma que perfome com mais robustez. \n",
    "\n",
    "Uma dessas formas é aplicando camadas de neuronios intermediarias entre os perceptrons iniciais e o resultado do output. Ao ser feita essa aplicação intermediaria, a rede consegue alcançar capacidades ainda maiores de discriminação, além de trazer aplicações para outros tipo de situações além da classificação binária, devido a sua capacidade de resolver problemas de maneira estocastica, como por exmeplo:\n",
    "\n",
    "* Reconhecimento de Padrões; \n",
    "* Identificação e controle de processos; \n",
    "* Previsão de Séries Temporais; \n",
    "* Otimização de sistemas;\n",
    "* Identificação de Imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então vendo pelo conceitos apresentados anteriormente sobre o perceptron, ele tem um processamente que chamado de *feedfoward*, onde o algoritmo segue um fluxo cadenciado da esquerda para a direita com a representação dos neurônios abaixo, o que faremos é incluir mais algumas camadas de perceptron entre os valores de entrada e os valores de saída. \n",
    "\n",
    "Mas a frente iremos compreender de que forma isso afeta o processamento dos dados utilizando do mesmo exemplo citado para o Perceptron de Rosenblatt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*piYTTh83qsQJVUMOZKmN5w.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olhando para a figura acima, temos diversos pontos de entrada para a rede que serão propagados pela redes inteira, da esquerda até a ponta da direita. Os primeiros perceptron irão fazer um disparar a frente se aquele ponto é relevante para a ativação e os neuronios seguintes irá fazer a combinação dos pontos de interesse das ativação geradas pelas camadas anteriores. Esse comportamente seguirá continuamento até o resultado final gerada pela rede. \n",
    "\n",
    "Então para deixarmos um pouco mais simples a compreensão desse funcionamento, voltemos ao exemplo do reconhecimento de algorismos em uma folha de papel. \n",
    "\n",
    "Então vamos recobrar alguns conceitos, para cada algorismo em uma folha de papel, o algoritmo irá interpretar cada pixel de uma imagem como uma função de ativação. Como demonstrada abaixo: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*GIcavxvEh3OtXqRLzfNa7Q.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planificando essa matrix com diversas pixeis em um vetor, teremos um vetor com 784 elementos, com um valor que pode representar o qual ativo se encontra esses píxeis de acordo com a força importa pelo pulso na folha de papel análisada. \n",
    "\n",
    "E vamos vamos aplicar mais uma camada de perceptron na nossa rede, então teremos algo semelhante a imagem abaixo: \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*gpipN1Hgf_W1kUJwhUEKYw.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada neurônio será ativado primariamente por cada pixel, como uma função binária, sendo 0 para desativado e 1 para ativado. \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*ks3dq8mitnYP70g16rn9LA.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém, já para a segunda camada, conforme apresentamos onde ela tá errando e onde está acertando ela pode começar a apresentar um comportamento de juntar o que dá certo e ignorar o que não está dando errado. \n",
    "\n",
    "Essa explicação pode expressar o comportamente para esse exemplo, mas não necessáriamente para outros casos, apesar de seu ponto forte se exatamente a alta capacidade de encontrar padrões que nós humanos não temos. \n",
    "\n",
    "Mas continuando, como o método pode juntar informação? Bom, acho que pra isso é mais facil demonstrar a imagem a seguir: \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*Kdt7I7GqRRZx-OMg85TDdA.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rede começa a fazer a junção de diversas junções das função de ativação anteriores e consequentemente ela tenta chegar a \"chute\" final no processamento. \n",
    "\n",
    "Então primeiramente pega ponto a ponto, e depois ela vai fazendo risco a risco até conseguir formular um chute. Então vamos dizer que adicionamos mais uma camada exatamente da representação geradaa acima. \n",
    "\n",
    "A rede pode fazer conclusões atravez de formas geométricas geradas pelas conclusões do perceptrons anteriores, como por exemplo: \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/700/1*YQDyrpikfBH1GOzIdBW-Kw.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o exemplo citado, não haveria a necessidade de incluir mais camadas, pois aumentariamos a complexidades dos calculos gerados e possívelmente a nossa rede pode se deparar com problemas como o *overfitting*, que é uma situação que desejamos esquivar completamente. \n",
    "\n",
    "Mas em geral, a quantidade de camadas intermediárias depende de diversos fatores da aplicação desejadas. Sendo também necessário ter noção de que nem sempre é necessária a aplicação de maiores quantidades de camadas, caso haja um bom ajustes dos pesos sinápticos, que é mais eficiente computacionalmente. \n",
    "\n",
    "Uma observação importante, é que para esse estudo estaremos fazendo uso do aprendizado observado, ou seja, sabemos os resultados que desejamos alcançar e podemos indicar para a máquina quando há um erro e quando há um acerto. O que facilita um pouco o treinamento dos receptores neurais com a técnica de *backpropagation* (algoritmo de retropropagação do erro) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
